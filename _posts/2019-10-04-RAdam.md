---
title: "Rectified Adam (RAdam) 정리"
date: 2019-10-04 15:48:28 -0400
categories: Deep_Learning
---

## Intro ##
딥러닝을 이용한 학습 시에 가중치를 업데이트하기 위해 꼭 필요한 Opimizer, 그 중에 가장 많이 사용되는 것은 누가 뭐래도 Adam optimizer일 것입니다.
그런데 이 Adam optimizer에도 사실 문제가 있고, 이 문제를 해결해 준 Adam의 변형이 Rectified Adam optimizer가 세상에 나왔습니다.
오늘은 이 Rectified Adam, 줄여서 RAdam으로 불리는 optimizer에 대해 포스팅하겠습니다.

## Adam optimizer의 한계점 ##
저처럼 단순히 'Adam을 쓰면 된다. 제일 낫다'라고만 들었던 사람들은 Adam이 가진 한계점이 무엇인지 잘 모를 수 있습니다.
사실 Adam의 한계점은 단순히 Adam에 한정되는 것이 아니라, Adaptiver learning rate을 사용하는 optimizer에서는 모두 발생하는 현상입니다.
이는 바로 'Bad local optima convergence problem'입니다. 이는 학습 초기에 샘플이 매우 부족하여 adaptive learning rate의 분산이 매우 커지고
이에 따라 최적이 아닌 local optima에 너무 일찍 도달하여 학습이 거의 일어나지 않는 현상입니다.
쉽게 생각해보면 처음 무언가를 배운다고 시작하면 보는 것마다 엄청 낯설어 혼란스러워 한다고 볼 수 있겠죠??

![gradients](https://i.imgur.com/7BoV1yq.png)

위 그림과 같이, Adam optimizer의 경우 학습 초기를 조금만 지나면 gradients의 값이 e^-20과 같이 매우매우매우 작은 값으로 변하는 것을 확인할 수 있습니다.
다른 말로 하면 초기에 어떠한 bad local optima에 수렴해버리기 때문에 그 이상 학습이 일어나지 않는 것임을 확인할 수 있습니다.

## Warmup heuristic ##
이러한 학습 초기 convergence problem을 해결하기 위해 다양한 시도들이 있었고, 최근 각광받는 방법은 바로 warmup heuristic이었습니다. 이는 말그대로
학습 초기에 warmup이 필요하다는 것입니다. 예를 들어 내가 정해준 learning rate가 0.01이라고 한다면 처음 10 step 동안은 0.001부터~ 0.01까지 선형적으로
증가하는 learning rate을 사용하는 것입니다. 이는 반대로 말하면 샘플이 거의 적은 초기에는 아주아주 작은 learning rate을 사용함으로써 bad local optima
로의 학습이 일어나지 않게 만드는 것이었습니다. 하지만 이러한 방식의 단점은 warmup step을 따로 정해줘야하기 때문에 찾아야하는 hyperparmeter가 하나 더 생기는 것


## Polynomial regression ##


##  ##



## Conclusion ##
우리는 Machine learning으로 rating을 예측하는 것이기 때문에 target과의 MSE error를 loss로 정하고 일반적인 학습 과정을 거쳐 global bias, 선형 회귀 weights, V 행렬의 값들을(parameters) 업데이트 해주면 된다. 추가적으로 feature가 latent vector를 하나만 가지는 문제를 해결한 FFM(Field aware Factorization Machine)방식도 있다고 하니 시간이 나면 찾아봐야겠다.
